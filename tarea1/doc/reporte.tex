\documentclass {article}

\usepackage[spanish]{babel}
\usepackage [T1]{fontenc}
\usepackage [utf8]{inputenc}
\usepackage {graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{multirow}

\begin {document}

\title{Tarea1: Simulación de cache}
\author{Daniel García Vaglio B42781, Esteban Zamora Alvarado B47769}
\maketitle
%\begin{abstract}

%\end{abstract}


\section{Protocolo MESI}
El protocolo MESI toma su nombre de las banderas que se utilizan para lograr la coherencia entre
caches del mismo nivel y caches de niveles distintos, que
indican el estado del bloque. Estas banderas son: ``Invalid'', ``Exclusive'', ``Shared'' y
``Modified''(están presentes en cada bloque). \textit{Invalid} implica que el bloque se encuentra inválido, entonces no se pueden hacer
lecturas del dato. \textit{Exclusive} se refiere a que el bloque se encuentra válido en esta unidad de cache,
pero no se encuentra válido en ningún ninguna otra. Luego \textit{Shared} se refiere a que el bloque se
encuentra válido en la presente unidad de cache, pero también puede estar válido en alguna otra
unidad de caché. Finalmente \textit{Modified} se utiliza para señalar que el bloque se
encuentra válido en el presente cache, pero que a diferencia de \textit{Exclusive} y \textit{shared}, este dato no
coincide con el dato en memoria RAM.

Este protocolo es utilizado para la coherencia de los datos en cache, ya que con el sistema de
banderas evita que se lea un dato que no es válido (que haya sido escrito por otro Core, y se lee el
dato sin mutar), y también garantiza que los datos mutados en cache, se guardan en la memoria RAM
adecuadamente.

En cuanto a la implementación, se deben considerar las instrucciones que el cache puede recibir,
estas vienen de dos partes, del core y del bus. El bus es un módulo que está conectado a todos los
caches del mismo nivel y se encarga de mantener los datos coherentes entre ellos. En primer lugar
se tiene que tomar en cuenta que el cache recibe instrucciones desde el core, en el que trata de
accesar a un bloque específico. Dependiendo de la bandera en el bloque el bus debe ejecutar
instrucciones específicas, a continuación se muestra un resumen:
\begin{center}
 \begin{tabular}{|c | c | l |} 
 \hline
 Instrucción & Bandera & Acción \\ 
 \hline
   \multirow{10}{*}{Core Read}  & \textit{Modified} & \textbullet Estado permanece igual \\
                               &          & \textbullet Hit \\ \cline{2-3}
   
             & \textit{Exclusive} & \textbullet Estado permanece igual \\
             &           & \textbullet Hit \\ \cline{2-3}
 
             & \textit{Shared} & \textbullet Estado permanece igual  \\
             &        & \textbullet Hit \\ \cline{2-3}
 
             & \textit{Invalid} & \textbullet Verificar si el dato está en otro cache\\
             &         & \textbullet Se hace fetch al nivel siguiente de cache\\
             &         & \textbullet Si está pasar a \textit{Shared}, si no pasar a \textit{Exclusive} \\ \cline{2-3}
 \hline
   \multirow{9}{*}{Core Write} & \textit{Modified} & \textbullet Estado permanece igual \\
             & & \textbullet Write Hit\\ \cline{2-3}
             & \textit{Exclusive} & \textbullet Estado cambia a \textit{Modified} \\
             & & \textbullet Write Hit\\ \cline{2-3}
             & \textit{Shared} & \textbullet Estado pasa a \textit{Modified} \\
             & & \textbullet Se invalidan copias del bloque en otros caches.\\
             & & \textbullet Write Hit\\ \cline{2-3}
             & \textit{Invalid}  & \textbullet Estado pasa a \textit{Modified}\\
   & & \textbullet Write Miss\\ \cline{2-3}
 \hline
 \end{tabular}
\end{center}


Los cores hacen peticiones de escritura o lectura de ciertos datos en cache, y estas pueden afectar
la coherencia, entonces el bus se encarga de coordinarlos. Por ejemplo es el bus es que se
encarga de buscar copias de un dato shared en los otros caches.

\section{Implementación}
Se toma la decisión de implementarlo en Python 2.7 que está disponible en la última versión de
ubuntu LTS. El script principal se llama ``main.py'' y tiene varios modos de funcionamiento que se
configuran con argumentos a la hora de ejecutarlo. Para una descripción breve de las funcionalidades
del script principal y su uso, se puede ejecutar ``python2 main.py --help''. Se puede cambiar los
archivos que se utilizan para las instrucciones de los cores, se puede activar la bandera de
``debug'' para recibir más información de la simulación, también al indicar un ``output\_file'' esta
información se guarda en un archivo en lugar de imprimirse en terminal, para permitir que se revise
después. Además se puede editar la razón de instrucciones que ejecuta cada core, por defecto el core
1 ejecuta 3 mientras el core 2 ejecuta 1, como se indica en las instrucciones de la tarea, pero estos parámetros
se pueden edita. También se ofrece la opción de ejecutar una simulación o varias, de manera que se puedan
hacer varias pruebas separadas con una sola ejecución del script.

Otra funcionalidad es la de
generar archivos de instrucciones aleatorias. De esta manera se crean archivos de texto con el
formato necesario para ser leído para las simulaciones. Estas instrucciones no son completamente
aleatorias, sino que se organizan en ``mega clusters'' y ``clusters'' de instrucciones. Los ``mega
clusters'' son grandes secuencias de direcciones aleatorias que pertenecen a un rango amplio para modelar el
comportamiento de un programa. Por otra parte los clusters son secuencias cortas de instrucciones
con direcciones cercanas, que modelan el comportamiento de funciones dentro de un programa. Los
archivos generados se pueden utilizar por el script principal para luego hacer simulaciones, y
tienen el objetivo de probar varios casos de entradas posibles, y también para ser utilizados en el
análisis de misses y hits del sistema de caches. para cada caso se selecciona de forma aleatoria si
se hace una lectura o una escritura. 

\subsection{Script principal de simulación}
Como se indicó con anterioridad el script principal de simulación puede hacer una o varias
simulaciones. El proceso es muy parecido, solo que cuando hay varias simulaciones los archivos de
entrada con las instrucciones para los cores se dividen en partes iguales y se pasan a cada
simulación por aparte. En adelante ambos casos son equivalentes.

Primero se realiza la lectura de los archivos, y se traducen del texto plano a una estructura de
datos más fácil de manejar para Python. Cada línea del archivo se convierte en las entradas de una
lista que contiene la dirección y si se trata de una lectura o una escritura. Además las
direcciones se guardan como strings que representan al número en binario, y se le agregan tantos
ceros al frente como sean necesarios para que tengan una tamaño mínimo específico. Esto se realiza
para facilitar el manejo de los datos en las siguientes etapas de la simulación.


Para poder emular la concurrencia que sucede en el sistema real se utilizan distintos procesos que se
ejecutan en paralelo. Python ofrece una interfaz de mensajes para poder intercomunicar procesos, por
lo que se utilizan a modo de puertos de comunicación. De esta manera, por ejemplo los datos que se
piden de memoria llegan por un puerto, y las instrucciones que se envían a cache van en otro puerto
distinto. Para una simulación se levantan 5 procesos, uno es el proceso de los cores estos se
encargan de leer la lista de instrucciones descrita anteriormente y de comunicarse con los caches
L1 de manera apropiada. Luego está el proceso de cache L1, este se encarga de la simulación de
ambos caches L1, y es donde se implementa el protocolo MESI. Otro proceso es el del cache L2, en este
caso se implementa el cache de segundo nivel. Luego está el proceso de la memoria, este se encarga
de simular el comportamiento de la memoria RAM en este sistema. Finalmente se tiene un quinto
proceso que se encarga de recibir los datos de todos los anteriores y organizarlos para hacer
impresiones en terminal con formato entendible y se encarga de generar los archivos de salida en caso
que el usuario así lo solicite. Es importante destacar que este quinto proceso no es parte de la
simulación de hardware, sino que es un proceso utilitario que permite la verificación del sistema
con mayor facilidad.

EN el script principal se crean todos los puertos de comunicación necesarios entre procesos. Los
puertos de comandos desde los niveles superiores hacia los niveles inferiores, y los puertos de
transferencia de datos se crean por aparte. Todos se organizan en un diccionario. Estos puertos y
otros parámetros utilitarios son los que se utilizan para inicializar los 5 procesos descritos.

Una simulación termina cuando ambos cores han terminado de ejecutar todas las instrucciones, en este
caso el proceso de los cores genera una señal para indicarle al resto que deben terminar. El proceso
de los cores termina automáticamente. Es en este momento que se hace el manejo de los estados
finales de los caches.

\subsection{Cores}

El proceso de los cores recibe como entrada ambas listas de instrucciones. Luego itera sobre ellas
sucesivamente hasta que ambas se hayan completado. En cada iteración uno de los cores envía las
instrucciones a su respectivo cache L1. En caso de ser necesario genera datos aleatorios para
enviarlos al cache, o espera a recibir los datos del cache. 


\subsection{Cache L1}
El módulo del cache L1 que crea dos objeto tipo cacheL2w con los tamaños requeridos y los puertos de
salida y entrada. Luego ejecuta los métodos que se encargan de escuchar las entradas y generar las
salidas. Este método es un loop que se espera a recibir una instrucción del nivel superior, esta
llega por el puerto de instrucciones.

Los datos en cache se guardan en un diccionario, donde el identificador es un string con el index en
binario, y este se asocia a un objeto tipo Block\_pair. Este objeto simula el comportamiento LRU del
set (El cache es 2 way associative LRU). Cada Block\_pair contiene dos objetos tipo Block\_MESI que
guardan los datos asociados al bloque y el tag, además se encargan de la transición de estados. 

El proceso de ejecución de una instrucción se divide en dos partes generales. La primera parte se
encarga del manejo de misses, y la segunda se encarga de la ejecución de la instrucción como
tal. Entonces al recibir una instrucción, el cache toma las instrucciones y las divide en el tag, el
index y el offset. Para esta parte es muy conveniente que se tomen las direcciones en formato
binario, para facilitar la división de la instrucción. Luego se busca en el diccionario el bloque
que contenga el tag requerido. En caso que no se encuentre se activa la bandera de miss y se ejecuta
la función para manejo de misses.

En la función de manejo de misses, primero se revisa el estado del bloque actual, encaso de estar en
\textit{Modified} se hace un flush del bloque para mantener la coherencia entre el cacheL1 y el cache
L2. Luego se busca el tag en el otro cache L1, para esto cada cache tiene un puntero al otro. En
caso de encontrarse el tag, se trae el dato. Si el dato en el otro cache se encuentra en \textit{Modified} se
hace flush, para garantizar coherencia y finalmente ambos bloques transicionan a \textit{Shared}. Si el tag
no se encuentra en el otro cache L1, entonces se le envía una solicitud al cache L2 por el
bloque, en este caso el bloque termina en \textit{Exclusive}. Nótese que todos los bloques inválidos levantan
la bandera de miss. 

Una vez que se ha ejecutado la función de manejo de misses se ha asegurado que se tiene coherencia
entre caches y con el cache L2. Además se garantiza que el bloque que se necesita se encuentra en el
cache L1 apropiado. Luego se ejecuta la instrucción del core. En caso de ser una lectura se pasa el
byte al cache por medio de un puerto. En caso de ser de escritura se toma el dato del puerto y se
escribe en el bloque. Este pasa a ser \textit{Modified}, y en caso de estar en \textit{Shared} entonces se invalida
la copia en el otro cache. Se debe notar que durante la ejecución de la instrucción el bloque puede
cambiar varias veces de estado, esto porque el manejo de misses y la ejecución de instrucciones del
core se hacen por separado.

\subsubsection{Block pair}

Como este es un cache two way set associative, entonces en cada entrada del diccionario de indexes en
el cache se encuentran dos bloques. Cada vez que el core accesa datos de alguno de ellos (ni importa
si es escritura o lectura), se le suma un contador al bloque correspondiente. Cuando los datos se
accesan por parte de otro cache, este contador no se incrementa. Cuando se trae en bloque nuevo al
cache, este inicia con el contador en 0. Estos contadores se utilizan para implementar LRU. Entonces
en un miss cuando se solicita el bloque LRU para ser sustituido, se retorna el bloque con el
contador menor valor.

\subsubsection{Block MESI}

Este es un objeto donde se implementa el protocolo MESI, en el sentido que se encarga de identificar
las instrucciones que se le solicitan al bloque y hacer la transición apropiada de estados.
%
%FIXME: EXPANDIR SOBRE IMPLEMENTACION. 
%

\subsection{CacheL2}

El protocolo MESI fue creado par asegurar la coherencia de datos entre \textbf{2 o más} caches del
mismo nivel. Como este cache es único, entonces no tiene sentido utilizar MESI. en su lugar se
utilizan tres banderas que indican si el dato es inválido, es válido (coherente con memoria
principal), o si está modificado (válido, pero distinto al dato en memoria). Este cache al igual que
el anterior tiene una función que se ejecuta indefinidamente y está escuchando el puerto de comandos
para saber cuando alguno de los cache L1 le hace una solicitud de un bloque o hace un flush de un
bloque.

Este cache se organiza como un diccionario, donde el identificador es un string con los indexes en
representación binaria y se asocia a un objeto tipo bloque. Este objeto tipo bloque almacena los
datos y el tag correspondiente. Sin embargo como la transición de estados es tan simple, esta se
maneja por parte del cache, y no por parte del bloque.

Cuando se recibe una instrucción del cache L1, primero se realiza la división de la dirección en
tag, index y offset. Se debe notar que esta división es distinta a la que se tiene en cache
L1. Además la parte del offset son solamente ceros, porque los bloques son del mismo tamaño. La
ejecución de la instrucción también se divide en dos partes: el manejo de misses, y la ejecución de
la instrucción como tal. En primer lugar se busca el tag correspondiente dentro del cache, si se
encuentra se procede a ejecutar la instrucción, pero en caso contrario se ejecuta la función de
manejo de misses.

En la función de manejo de misses primero se hace flush del bloque actual a la memoria principal,si
este se encuentra modificado, y
luego se hace la solicitud a la memoria del bloque que necesita cache L1. Este dato se trae y se
guarda en el cache L2, se asigna bandera de válido.

En al parte de la ejecución de la instrucción recibida por parte del cache L1, en caso de ser de
lectura se envían los datos del bloque, en caso de ser escritura, se toman los datos y se el asigna
la bandera de modificado.

\section{Memoria Principal}

La memoria principal es un proceso que se encarga de simular el comportamiento de la memoria RAM en
este sistema. Al igual que los caches se mantiene escuchando el puerto de comandos en espera que el
nivel superior le haga una solicitud. Esta memoria tiene un diccionario, donde guarda la dirección
asociada y la socia al dato (lista de 32 bytes).  Estos se generan en cada solicitud del cacheL2. En
caso de haber una lectura, la memoria primero busca en el diccionario si ese dato ya está
disponible y lo envía, si no lo está entonces crea uno aleatorio, lo guarda y lo envía. Si el
comando es de escritura, busca si el dato ya está disponible y los modifica, si no, entonces crea
una entrada nueva, y lo guarda.

\section{Consideraciones sobre comandos}
Se ha explicado que los niveles superiores envían comandos a los niveles inferiores. El ejemplo más
claro es la relación entre el core y el cache L1. Este comando es una lista de dos elementos, el
primero indica la dirección, y el segundo indica si es escritura o lectura. Entonces es parecido a
este ejemplo: [``0101...101'', ``{S}'']. En caso que haya un miss, y se deba hacer un flush, entonces el
cache L1 va a enviarle al cache L2 la siguiente instrucci\'o n: [``1001...000'', ``{S}''], para escribir
el bloque a sustituir en L2. Y luego va a enviar [``0101...000'', ``{L}''], para leer el bloque
correspondiente. Note que internamente todo se trabaja en binario, no hexadecimal, además escritura
o lectura se indica encerrado en llaves. Por otra parte el offset que se trabaja del cacheL2 para
abajo es siempre cero, porque estas transferencias son de bloques enteros. Las transferencias entre
los cores y sus caches L1 son únicamente de bytes (para esta implementación las palabras son de un byte). 


\section{Funcionamiento y Pruebas}

\subsection{Miss rate y Hit rate}

Para analizar de manera sencilla las propiedades de este sistema, se hace una prueba donde se mide
el miss rate y miss rate de cada nivel de cache. Para esto se generan dos archivos de instrucciones
aleatorias con el siguiente comando: ``python2 main.py
--generate\_random=100000,rnd.txt,100000,rnd2.txt''. Cada uno con 100000 instrucciones. Cabe destacar
que estos archivos dentro contienen los clusters y mega clusters que se explicaron en la sección de
implementación. 


Para realizar las pruebas se ejecuta ``python2 main.py --core1\_file=rnd.txt --core2\_file=rnd2.txt
--debug --ratio=1:1 --run\_n=10''. En caso de ejecutar esta prueba se debe tomar en cuenta que tuvo
una duración aproximada de 52 minutos. De esta manera se ejecutan 10 pruebas con 9999 instrucciones cada
una. Se obtiene que el miss rate total para el cache L1 es de 0.00365, que es incluso menor al 1\%
. Esto indica que L1 es sumamente eficiente en el manejo de misses. En este caso el protocolo MESI
no tiene ninguna contribución a disminuir tanto el miss rate, sino que esta disminución se logra por
que el cache se implementa como 2-way set associative LRU. 

En el cache L1 se mide la cantidad de misses que se
resuelven buscando en el otro cache L1, y por separado la cantidad de misses que se resuelven
pidiendo el dato a L2. Se tiene que el miss rate resuelto en el otro cache es de 0, mientras que el
miss rate resuelto en L2 es 0.00365. Entonces la gran mayoría de misses se resuelven en el cache
L2. Esto se debe a que en esta prueba aleatoria no existe localidad espacial entre cores, es decir
ambos cores se encuentran operando con direcciones distantes, y por tanto los datos en los caches L1
no son cercanos.

Luego se debe tomar en cuenta también el miss rate de cache L2. Este tiene un miss rate de 1, lo que
implica que la gran mayoría de las peticiones resultan en un miss. Esto se debe a que el tamaño del
bloque del cache L1 y del cache L2 es el mismo, entonces la capacidad de manejar localidad espacial
de ambos niveles del cache es sumamente similar. Entones los misses que ocurren en L1 porque no tiene
suficiente manejo de localidad espacial, también van a suceder en cache L2. Por otra parte se debe
tomar en cuenta que L2 debe manejar peticiones de ambos cache L1, que como ya se demostró trabajan
con direcciones distintas, esta situación puede estar generando conflictos en L2, que le provoquen
realizar flushes de sus datos a la memoria principal, y por tanto misses. Finalmente respecto a la
localidad temporal, como el cache L1 es 2-way LRU podría tener hasta mejor localidad temporal que
L2. L2 en lo único que es mejor que L1 es en su tamaño, por lo que puede contener más direcciones
distantes, esto ayuda a evitar misses por conflicto.

Por otra parte cuando se corre al simulación con los datos propuestos por el profesor, se obtienen
resultados distintos. El miss rate de L1 total es de 0.247, que es peor que en el caso
aleatorio. Por otra parte el miss rate que se soluciona en otro cache aumentó a 0.1095, lo que
implica que alrededor del 10\% de instrucciones son misses que se resuelven en el otro cache, esto
indica un mayor acople entre las instrucciones que ejecutan ambos cores. Luego
el miss rate que se resuelve en L2, aumentó a 0.1375, entonces en general se tienen más accesos a L2
que en el caso aleatorio. Finalmente el miss rate de L2 es mucho menor, para este caso es de 0.478,
lo que implica que se aprovecha mejor la característica de L2 de ser más grande. 

\section{Beneficios de MESI}

El principal beneficio es que permite tener coherencia entre varios caches del mismo nivel, y con
los caches del siguiente nivel, ya que esta es al razón por la que se creó este protocolo. El caché
más básico únicamente tiene la bandera de invalid, esta bandera permite que no se envíen datos al
core o a memoria que no tienen sentido.

Sin embargo si el cache solamente tiene invalid, entonces se
va a necesitar hacer un flush en cada miss, lo que incrementa el miss penalty. Para solucionar este
problema se utiliza la bandera de Modified. De esta manera el sistema sabe cuando el dato en cache
ha cambiado respecto al dato en la memoria del siguiente nivel, de manera que si no ha cambiado,
cuando suceda un miss no se debe hacer el flush. Modified Permite asegurar coherencia con los
siguientes niveles, y ayuda a disminuir el miss penalty.

Por otra parte la bandera shared se utiliza para la coherencia de datos cuando se tienen dos o más
caches en el mismo nivel. En el caso de que un dato compartido sea modificado se deben eliminar las
copias del dato en los otros caches, para no permitirle a los otros cores accesar a datos
erróneos. De esta manera se obtiene una sincronización entre los caches, y en caso que otro core
necesite el valor modificado, entonces se toma desde el cache, y no es necesario pedirlo al
siguiente nivel de cache.

El problema con estas banderas es que constantemente se encuentran haciendo accesos al bus que
interconecta los caches. Esto puede tener un efecto negativo en el miss penalty. Además a la hora de
modificar un dato shared se debe buscaren el resto de caches para invalidar copias, por lo que se
pueden tener efectos negativos en el hit time. Con la adición de la bandera exclusive, se pude
resolver parte del problema, ya que se indica que los datos exclusive están únicamente en un cache y
no es necesario buscarlos en los otros caches. Esto ahorra muchas transacciones en el bus. Cabe
destacar que en las pruebas aleatorias los caches del nivel L1 casi no compartían datos, entonces casi
no habían bloque sen estado shared.Por lo que en ese caso particular la bandera de Exclusive ayuda a
aumentar mucho la eficiencia del sistema.

En general el protocolo MESI, provee los beneficios que se espera de un protocolo de coherencia. La bandera I ayuda a que no se envíen al core o a memoria datos sin
sentido. La bandera M ayuda a disminuir la cantidad de flushes, y por tanto a reducir el miss
rate. La bandera S permite coherencia entre caches del mismo nivel, y finalmente la bandera E ayuda
a disminuir los acceso al bus y por tanto ayudan a mejorar el miss rate y el hit time. 

\section{Deficiencias de MESI}

MESI es un protocolo que permite de manera satisfactoria garantizar la coherencia de los datos. Sin
embargo aun con el aditamento de la bandera de modified, suceden varios flushes que no son
necesarios, principalmente con los bloques que se encuentran shared. Ya que al modificar un bloque
en estado shared, se deben invalidar todas sus copias, lo que provoca que hayan posibles misses
para otros cores. Entonces al modificar bloques en shared, se aumenta el miss rate del sistema. Esta
medida del protocolo MESI no toma en consideración si el dato se ha utilizado más en
los otros caches (y por lo tanto probablemente se necesite nuevamente), tampoco permite
sincronización dentro del mismo nivel de caches, sino que definitivamente se requiere de acceso al siguiente nivel
de memoria para resolver los misses (el dato modified necesita flush, para ser enviado a otros
caches). Entonces no solo se tiene un mal miss rate, sino también un mal miss penalty. MESI necesita
mejorarse en el manejo de modificaciones a datos en shared.  


\end{document}
